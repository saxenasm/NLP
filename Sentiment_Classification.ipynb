{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Sentiment Classification on Coronavirus Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.feature_extraction.text import CountVectorizer , TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import \\\n",
    "classification_report, accuracy_score,confusion_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model using tf-idf and bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= pd.read_csv(\n",
    "    '/Documents/kaggleDatasets/coronavirus_tweets/Corona_NLP_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>44953</td>\n",
       "      <td>NYC</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>44954</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>44955</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>Extremely Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>44956</td>\n",
       "      <td>Chicagoland</td>\n",
       "      <td>02-03-2020</td>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>44957</td>\n",
       "      <td>Melbourne, Victoria</td>\n",
       "      <td>03-03-2020</td>\n",
       "      <td>#toiletpaper #dunnypaper #coronavirus #coronav...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName             Location     TweetAt  \\\n",
       "0         1       44953                  NYC  02-03-2020   \n",
       "1         2       44954          Seattle, WA  02-03-2020   \n",
       "2         3       44955                  NaN  02-03-2020   \n",
       "3         4       44956          Chicagoland  02-03-2020   \n",
       "4         5       44957  Melbourne, Victoria  03-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  TRENDING: New Yorkers encounter empty supermar...  Extremely Negative  \n",
       "1  When I couldn't find hand sanitizer at Fred Me...            Positive  \n",
       "2  Find out how you can protect yourself and love...  Extremely Positive  \n",
       "3  #Panic buying hits #NewYork City as anxious sh...            Negative  \n",
       "4  #toiletpaper #dunnypaper #coronavirus #coronav...             Neutral  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "tokenizer =ToktokTokenizer()\n",
    "#stopwords removal\n",
    "stopword_list= nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remving hmtl strips\n",
    "def strip_html(text):\n",
    "    soup= BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    pattern= r'[^a-zA-Z0-0\\s]'\n",
    "    text=re.sub(pattern, '', text)\n",
    "    return text\n",
    "def simple_stemmer(text):\n",
    "    ps= nltk.porter.PorterStemmer()\n",
    "    text=' '.join([ps.stem(word) for word in \\\n",
    "                   text.split()])\n",
    "    return text\n",
    "\n",
    "stop= set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens= tokenizer.tokenize(text)\n",
    "    tokens= [token.strip() for token in tokens]\n",
    "    filtered_tokens= [token for token in tokens \\\n",
    "                      if token \\\n",
    "                      not in stopword_list]\n",
    "    filtered_text= ' '.join(filtered_tokens)\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply preprocessing functions\n",
    "df=dataset.copy()\n",
    "\n",
    "#do convert to lowercase before preprocessing\n",
    "df['text']=df['OriginalTweet'].str.lower()\n",
    "\n",
    "df['text']=df['text'].apply(strip_html)\n",
    "\n",
    "df['text']=df['text'].apply(remove_special_characters)\n",
    "\n",
    "df['text']=df['text'].apply(simple_stemmer)\n",
    "\n",
    "df['text']=df['text'].apply(remove_stopwords)\n",
    "\n",
    "#df.text has now all the preprocessing steps applied sequentially \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encode the taregt column, it has 5 classes\n",
    "df['Sentiment']=df['Sentiment'].astype('category')\n",
    "df['Sentiment_coded']= df.Sentiment.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3798, 8)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle dataset at random\n",
    "df= df.sample(df.shape[0])\n",
    "\n",
    "#train tets split\n",
    "xtrain, ytrain, xtest, ytest = df.text[:3000], \\\n",
    "            df.Sentiment_coded[:3000],\\\n",
    "            df.text[3000:], \\\n",
    "            df.Sentiment_coded[3000:]\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_cv_train (3000, 94019)\n",
      "bow_cv_test (798, 94019)\n"
     ]
    }
   ],
   "source": [
    "#count vectorizer for bag of words\n",
    "cv= CountVectorizer(min_df=0, max_df=1 ,\\\n",
    "                    ngram_range=(1,3))\n",
    "\n",
    "#transformed train data\n",
    "cv_train= cv.fit_transform(xtrain)\n",
    "\n",
    "#transform test data\n",
    "cv_test= cv.transform(xtest)\n",
    "\n",
    "print('bow_cv_train',cv_train.shape)\n",
    "print('bow_cv_test', cv_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf_train (3000, 94019)\n",
      "tfids_test (798, 94019)\n"
     ]
    }
   ],
   "source": [
    "#tfidf vectorizer\n",
    "tv= TfidfVectorizer(min_df=0, max_df=1, use_idf=True, ngram_range=(1,3))\n",
    "#transform train data\n",
    "tv_train= tv.fit_transform(xtrain)\n",
    "\n",
    "#transform text data\n",
    "tv_test= tv.transform(xtest)\n",
    "\n",
    "\n",
    "print('tfidf_train',tv_train.shape)\n",
    "print('tfids_test', tv_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training model\n",
    "lr= LogisticRegression(penalty='l2',max_iter=50,\\\n",
    "                       C=1, random_state=42)\n",
    "\n",
    "#fitting model for bow\n",
    "lr_bow= lr.fit(cv_train, ytrain)\n",
    "# print(lr_bow)\n",
    "\n",
    "#fititng model for tfidf\n",
    "lr_tfidf= lr.fit(tv_train, ytrain)\n",
    "# print(lr_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting model for bow\n",
    "lr_bow_predict= lr_bow.predict(cv_test)\n",
    "\n",
    "#predict for tfidf\n",
    "lr_tfidf_predict= lr_tfidf.predict(tv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.05      0.09       130\n",
      "           1       0.42      0.04      0.08       114\n",
      "           2       0.30      0.75      0.43       224\n",
      "           3       0.57      0.06      0.11       133\n",
      "           4       0.30      0.32      0.31       197\n",
      "\n",
      "    accuracy                           0.31       798\n",
      "   macro avg       0.49      0.24      0.20       798\n",
      "weighted avg       0.45      0.31      0.24       798\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.02      0.05       130\n",
      "           1       1.00      0.02      0.03       114\n",
      "           2       0.29      0.85      0.43       224\n",
      "           3       0.83      0.04      0.07       133\n",
      "           4       0.28      0.19      0.23       197\n",
      "\n",
      "    accuracy                           0.30       798\n",
      "   macro avg       0.68      0.22      0.16       798\n",
      "weighted avg       0.60      0.30      0.20       798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report \n",
    "report_bow= classification_report(ytest,\\\n",
    "                                  lr_bow_predict)\n",
    "\n",
    "print(report_bow)\n",
    "\n",
    "report_tfidf= classification_report(ytest,\\\n",
    "                                    lr_tfidf_predict)\n",
    "print(report_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6   0 101   2  21]\n",
      " [  0   5  65   1  43]\n",
      " [  0   4 167   2  51]\n",
      " [  0   1  92   8  32]\n",
      " [  1   2 130   1  63]]\n",
      "[[  3   0 117   0  10]\n",
      " [  0   2  82   0  30]\n",
      " [  0   0 190   0  34]\n",
      " [  0   0 103   5  25]\n",
      " [  0   0 158   1  38]]\n"
     ]
    }
   ],
   "source": [
    "cm_bow=confusion_matrix(ytest, lr_bow_predict)\n",
    "print(cm_bow)\n",
    "\n",
    "cm_tfidf= confusion_matrix(ytest, lr_tfidf_predict)\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network for tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert sparse representaion to dense for tfidf matrix\n",
    "xtrain = scipy.sparse.csr_matrix.todense(tv_train)\n",
    "xtest = scipy.sparse.csr_matrix.todense(tv_test)\n",
    "# ytrain= ytrain\n",
    "# ytest=ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 94019)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with scipy.sparse representaion no need to specify the dim of input layer\n",
    "\n",
    "nn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(48, activation='relu', \n",
    "                    input_shape=(tv_train.shape[1],)),\n",
    "   tf.keras.layers.Dropout(0.2),\n",
    "   tf.keras.layers.Dense(24, activation='relu'),\n",
    "   tf.keras.layers.Dropout(0.1),\n",
    "   tf.keras.layers.Dense(5, \n",
    "                         activation='softmax')\n",
    "])\n",
    "#last layer is softmax as it is multiclass classification\n",
    "#numbr of nodes in last layer= number of classes in target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sparse crosentropy loss as most values in tfidf matrix is zero\n",
    "\n",
    "nn_model.compile(loss='sparse_categorical_crossentropy'\n",
    "              , optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94/94 - 14s - loss: 1.5968 - accuracy: 0.2637 - val_loss: 1.5799 - val_accuracy: 0.2820\n",
      "Epoch 2/10\n",
      "94/94 - 3s - loss: 1.4564 - accuracy: 0.3947 - val_loss: 1.5594 - val_accuracy: 0.2957\n",
      "Epoch 3/10\n",
      "94/94 - 4s - loss: 0.7890 - accuracy: 0.8023 - val_loss: 1.5696 - val_accuracy: 0.3108\n",
      "Epoch 4/10\n",
      "94/94 - 5s - loss: 0.1992 - accuracy: 0.9847 - val_loss: 1.6455 - val_accuracy: 0.3045\n",
      "Epoch 5/10\n",
      "94/94 - 5s - loss: 0.0535 - accuracy: 0.9967 - val_loss: 1.7167 - val_accuracy: 0.3070\n",
      "Epoch 6/10\n",
      "94/94 - 5s - loss: 0.0307 - accuracy: 0.9970 - val_loss: 1.7630 - val_accuracy: 0.2982\n",
      "Epoch 7/10\n",
      "94/94 - 6s - loss: 0.0217 - accuracy: 0.9970 - val_loss: 1.8006 - val_accuracy: 0.3008\n",
      "Epoch 8/10\n",
      "94/94 - 4s - loss: 0.0150 - accuracy: 0.9977 - val_loss: 1.8554 - val_accuracy: 0.2932\n",
      "Epoch 9/10\n",
      "94/94 - 4s - loss: 0.0138 - accuracy: 0.9970 - val_loss: 1.8782 - val_accuracy: 0.2970\n",
      "Epoch 10/10\n",
      "94/94 - 6s - loss: 0.0126 - accuracy: 0.9973 - val_loss: 1.9225 - val_accuracy: 0.2870\n"
     ]
    }
   ],
   "source": [
    "history = nn_model.fit(x_train, y_train, epochs=10,\n",
    "                    validation_data=(x_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting model prediction\n",
    "test_pred_nn_tfidf =  nn_model.predict(xtest)\n",
    "\n",
    "#getting index of maximum probability\n",
    "test_pred_nn_tfidf=[ np.argmax(x)\\\n",
    "                    for x in test_pred_nn_tfidf]\n",
    "#classification report\n",
    "report_nn_vector= classification_report(ytest,\\\n",
    "                                test_pred_nn_tfidf)\n",
    "\n",
    "print(report_nn_vector)\n",
    "\n",
    "#confusion matrix\n",
    "cm_nn_vector= confusion_matrix(ytest, test_pred_nn1)\n",
    "\n",
    "print(cm_nn_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model using pretrained models and logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_url=\n",
    "\"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "model= hub.load(model_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(input):\n",
    "    return model([input])\n",
    "\n",
    "#making one copy of dataset for pretrained models\n",
    "df1= dataset.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizing tweet column using universal snetence encoder\n",
    "df1['text_vector']= df1.OriginalTweet.map(embed)\n",
    "df1['text_vector']= df1.text_vector.map(np.array)\n",
    "\n",
    "#save transformed data locally \n",
    "pickle.dump(df1, open('dataset_covid_tweets.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding dsentiment column \n",
    "df1['Sentiment']=df1['Sentiment'].astype('category')\n",
    "df1['Sentiment_coded']= df1.Sentiment.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle dataset at random\n",
    "df1= df1.sample(df1.shape[0])\n",
    "\n",
    "#spiltting test and train data\n",
    "xtrain1, ytrain1, xtest1, ytest1 = \n",
    "            df1['text_vector'][:3000], \\\n",
    "            df1.Sentiment_coded[:3000],\\\n",
    "            df1['text_vector'][3000:], \\\n",
    "            df1.Sentiment_coded[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "(3000, 512)\n"
     ]
    }
   ],
   "source": [
    "#convert 2d list to 1 d list for text vector\n",
    "import itertools\n",
    "xtrain1= list(itertools.chain(*xtrain1))\n",
    "print(len(xtrain1))\n",
    "xtrain1=np.mat(xtrain1)\n",
    "print(xtrain1.shape)\n",
    "xtest1= list(itertools.chain(*xtest1))\n",
    "xtest1=np.mat(xtest1)\n",
    "\n",
    "#xtrain1 and xtest1 are lists of vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= LogisticRegression(penalty='l2',\\\n",
    "                max_iter=500,C=1, random_state=42)\n",
    "\n",
    "#fitting model for embedding vectors\n",
    "lr_vector= lr.fit( xtrain1, ytrain1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.42      0.46       120\n",
      "           1       0.53      0.42      0.47       123\n",
      "           2       0.35      0.48      0.40       201\n",
      "           3       0.56      0.53      0.54       140\n",
      "           4       0.40      0.36      0.38       214\n",
      "\n",
      "    accuracy                           0.44       798\n",
      "   macro avg       0.47      0.44      0.45       798\n",
      "weighted avg       0.45      0.44      0.44       798\n",
      "\n",
      "[[51  2 54  2 11]\n",
      " [ 9 52 23  5 34]\n",
      " [29 11 96 20 45]\n",
      " [ 5  5 34 74 22]\n",
      " [ 9 28 70 31 76]]\n"
     ]
    }
   ],
   "source": [
    "#predicting model for vector embedding\n",
    "lr_vector_predict= lr_vector.predict(xtest1)\n",
    "\n",
    "#classification report\n",
    "report_lr_vector= classification_report(ytest1,\\\n",
    "                                lr_vector_predict)\n",
    "\n",
    "print(report_lr_vector)\n",
    "\n",
    "#confusion matrix\n",
    "cm_lr_vector= confusion_matrix(ytest1,\\\n",
    "                            lr_vector_predict)\n",
    "\n",
    "print(cm_lr_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN Model on encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if data has (bacthsize*d2) then input layer should be of dim- d2\n",
    "#data here has 512 features , so input size is set to 512\n",
    "nn_model1 = tf.keras.Sequential([\n",
    "   tf.keras.layers.InputLayer(input_shape=(512,)),\n",
    "   tf.keras.layers.Dense(20, activation='relu', \n",
    "                input_shape=(tv_train.shape[1],)),\n",
    "   tf.keras.layers.Dropout(0.2),\n",
    "   tf.keras.layers.Dense(24, activation='relu'),\n",
    "   tf.keras.layers.Dropout(0.1),\n",
    "   tf.keras.layers.Dense(5, \n",
    "                         activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_46 (Dense)             (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 24)                504       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 5)                 125       \n",
      "=================================================================\n",
      "Total params: 10,889\n",
      "Trainable params: 10,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn_model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model1.compile(loss='sparse_categorical_crossentropy'\n",
    "              , optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "94/94 - 2s - loss: 1.5795 - accuracy: 0.2557 - val_loss: 1.5451 - val_accuracy: 0.2907\n",
      "Epoch 2/10\n",
      "94/94 - 0s - loss: 1.5170 - accuracy: 0.3100 - val_loss: 1.4812 - val_accuracy: 0.3471\n",
      "Epoch 3/10\n",
      "94/94 - 0s - loss: 1.4341 - accuracy: 0.3743 - val_loss: 1.4109 - val_accuracy: 0.3759\n",
      "Epoch 4/10\n",
      "94/94 - 0s - loss: 1.3561 - accuracy: 0.4213 - val_loss: 1.3701 - val_accuracy: 0.3922\n",
      "Epoch 5/10\n",
      "94/94 - 0s - loss: 1.3066 - accuracy: 0.4517 - val_loss: 1.3518 - val_accuracy: 0.4148\n",
      "Epoch 6/10\n",
      "94/94 - 0s - loss: 1.2765 - accuracy: 0.4550 - val_loss: 1.3481 - val_accuracy: 0.4185\n",
      "Epoch 7/10\n",
      "94/94 - 0s - loss: 1.2590 - accuracy: 0.4620 - val_loss: 1.3491 - val_accuracy: 0.3972\n",
      "Epoch 8/10\n",
      "94/94 - 0s - loss: 1.2302 - accuracy: 0.4760 - val_loss: 1.3460 - val_accuracy: 0.4135\n",
      "Epoch 9/10\n",
      "94/94 - 0s - loss: 1.2176 - accuracy: 0.4990 - val_loss: 1.3461 - val_accuracy: 0.4085\n",
      "Epoch 10/10\n",
      "94/94 - 0s - loss: 1.2136 - accuracy: 0.4847 - val_loss: 1.3535 - val_accuracy: 0.4135\n"
     ]
    }
   ],
   "source": [
    "history1= nn_model1.fit(xtrain1,\n",
    "                        ytrain1, epochs=10,\n",
    "                    validation_data=(xtest1, ytest1), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.46      0.51       120\n",
      "           1       0.62      0.42      0.50       114\n",
      "           2       0.46      0.61      0.53       219\n",
      "           3       0.58      0.55      0.57       134\n",
      "           4       0.48      0.47      0.48       211\n",
      "\n",
      "    accuracy                           0.52       798\n",
      "   macro avg       0.54      0.50      0.52       798\n",
      "weighted avg       0.53      0.52      0.51       798\n",
      "\n",
      "[[ 55   1  56   5   3]\n",
      " [  3  48  18   1  44]\n",
      " [ 28   6 134  22  29]\n",
      " [  0   1  26  74  33]\n",
      " [  8  22  56  25 100]]\n"
     ]
    }
   ],
   "source": [
    "#getting model prediction\n",
    "test_pred_nn =  nn_model1.predict(xtest1)\n",
    "\n",
    "#getting index of maximum probability\n",
    "test_pred_nn1=[ np.argmax(x) for x in test_pred_nn]\n",
    "#classification report\n",
    "report_nn_vector= classification_report(ytest1,\\\n",
    "                                        test_pred_nn1)\n",
    "\n",
    "print(report_nn_vector)\n",
    "\n",
    "#confusion matrix\n",
    "cm_nn_vector= confusion_matrix(ytest1, test_pred_nn1)\n",
    "\n",
    "print(cm_nn_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
